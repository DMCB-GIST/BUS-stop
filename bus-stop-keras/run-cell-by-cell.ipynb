{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "21:20:06:BUS-stop:INFO: ***Logging start***\n",
      "21:20:06:BUS-stop:INFO: os.environ['CUDA_VISIBLE_DEVICES'] = 0\n",
      "21:20:06:BUS-stop:INFO: devices = ['/device:GPU:0']\n",
      "21:20:06:BUS-stop:INFO: Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import preliminary\n",
    "import pt_modeler\n",
    "import preprocessing as pp\n",
    "\n",
    "from collections import deque\n",
    "from pt_modeler import ConstructPtModeler\n",
    "from huggingface_utils import MODELS\n",
    "from scipy.special import softmax\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import log_loss, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import cosine,euclidean\n",
    "\n",
    "logger = logging.getLogger('BUS-stop')\n",
    "formatter = logging.Formatter('%(asctime)s:%(name)s:%(levelname)s: %(message)s',\"%H:%M:%S\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "fhandler = logging.FileHandler(filename='./logs/run-cell-by-cell.log', mode='w')\n",
    "fhandler.setFormatter(formatter)\n",
    "fhandler.setLevel(logging.INFO)\n",
    "logger.addHandler(fhandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler(sys.stdout)\n",
    "consoleHandler.setFormatter(formatter)\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "#Variables for preprocessing\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "GLOBAL_SEED = 0\n",
    "pt_model = \"TFBertModel\"\n",
    "pt_model_checkpoint = \"./params/bert_base/\"\n",
    "\n",
    "for indx, model in enumerate(MODELS):\n",
    "    if model[0].__name__ == pt_model:\n",
    "        TFModel, Tokenizer, Config = MODELS[indx]\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(pt_model_checkpoint)\n",
    "\n",
    "devices = []\n",
    "for gpu_num in os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','):\n",
    "    devices.append('/device:GPU:{}'.format(gpu_num))\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(devices=devices)\n",
    "gpus = strategy.num_replicas_in_sync\n",
    "\n",
    "logger.info(\"***Logging start***\")\n",
    "logger.info(\"os.environ['CUDA_VISIBLE_DEVICES'] = {}\".format(os.environ['CUDA_VISIBLE_DEVICES']))\n",
    "logger.info(\"devices = {}\".format(devices))\n",
    "logger.info(\"Number of devices: {}\".format(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:20:17:BUS-stop:INFO: *******************\n",
      "21:20:17:BUS-stop:INFO: ***Preprocessing***\n",
      "21:20:17:BUS-stop:INFO: *******************\n",
      "21:20:17:BUS-stop:INFO: In file ./data/SST-2/labeled.tsv, we read 100 samples, \n",
      "21:20:17:BUS-stop:INFO: where the class distribution is {'0': 50, '1': 50, None: 0}.\n",
      "21:20:17:BUS-stop:INFO: In file ./data/SST-2/test_with_gold.tsv, we read 1821 samples, \n",
      "21:20:17:BUS-stop:INFO: where the class distribution is {'0': 912, '1': 909, None: 0}.\n",
      "21:20:17:BUS-stop:INFO: In file ./data/SST-2/unlabeled.tsv, we read 1821 samples, \n",
      "21:20:17:BUS-stop:INFO: where the class distribution is {'0': 0, '1': 0, None: 1821}.\n",
      "21:20:18:BUS-stop:INFO: Labeled//Test//Unlabeled matrix shape = (100, 64) // (1821, 64) // (1821, 64)\n",
      "21:20:18:BUS-stop:INFO: ***Train***\n",
      "21:20:18:BUS-stop:INFO: Example 0\n",
      "21:20:18:BUS-stop:INFO: Label 0\n",
      "21:20:18:BUS-stop:INFO: Token ids [  101  2062  1997  1996  2168  2214 13044  5365  2038  2042  2667  2000\n",
      "  3413  2125  2004 11701  9458  4024  2005  2070  2051  2085  1012   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "21:20:18:BUS-stop:INFO: Tokens ['[CLS]', 'more', 'of', 'the', 'same', 'old', 'garbage', 'hollywood', 'has', 'been', 'trying', 'to', 'pass', 'off', 'as', 'acceptable', 'teen', 'entertainment', 'for', 'some', 'time', 'now', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "21:20:18:BUS-stop:INFO: Token mask [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "21:20:18:BUS-stop:INFO: ***Train***\n",
      "21:20:18:BUS-stop:INFO: Example 1\n",
      "21:20:18:BUS-stop:INFO: Label 0\n",
      "21:20:18:BUS-stop:INFO: Token ids [ 101 2023 2047 3414 2746 1011 1997 1011 2287 3185 2003 1050 1005 1056\n",
      " 2428 2055 2505 1012  102    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "21:20:18:BUS-stop:INFO: Tokens ['[CLS]', 'this', 'new', 'zealand', 'coming', '-', 'of', '-', 'age', 'movie', 'is', 'n', \"'\", 't', 'really', 'about', 'anything', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "21:20:18:BUS-stop:INFO: Token mask [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "21:20:18:BUS-stop:INFO: ***Test***\n",
      "21:20:18:BUS-stop:INFO: Example 0\n",
      "21:20:18:BUS-stop:INFO: Label 0\n",
      "21:20:18:BUS-stop:INFO: Token ids [  101 15491 28616 22444  4095  1997  6782  1998 11541  1012   102     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "21:20:18:BUS-stop:INFO: Tokens ['[CLS]', 'uneasy', 'mis', '##hma', '##sh', 'of', 'styles', 'and', 'genres', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "21:20:18:BUS-stop:INFO: Token mask [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "21:20:18:BUS-stop:INFO: ***Test***\n",
      "21:20:18:BUS-stop:INFO: Example 1\n",
      "21:20:18:BUS-stop:INFO: Label 0\n",
      "21:20:18:BUS-stop:INFO: Token ids [  101  2023  2143  1005  1055  3276  2000  5025  6980  2003  1996  2168\n",
      "  2004  2054  4234  1011  3392 19311  2075  1999  1037 12509  2064  2003\n",
      "  2000  5025  4586  1024  1037  3532  1011  1011  2065 25634  1011  1011\n",
      " 20017  1012   102     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "21:20:18:BUS-stop:INFO: Tokens ['[CLS]', 'this', 'film', \"'\", 's', 'relationship', 'to', 'actual', 'tension', 'is', 'the', 'same', 'as', 'what', 'christmas', '-', 'tree', 'flock', '##ing', 'in', 'a', 'spray', 'can', 'is', 'to', 'actual', 'snow', ':', 'a', 'poor', '-', '-', 'if', 'durable', '-', '-', 'imitation', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "21:20:18:BUS-stop:INFO: Token mask [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "21:20:18:BUS-stop:INFO: ***Unlabeled***\n",
      "21:20:18:BUS-stop:INFO: Example 0\n",
      "21:20:18:BUS-stop:INFO: Token ids [  101 15491 28616 22444  4095  1997  6782  1998 11541  1012   102     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "21:20:18:BUS-stop:INFO: Tokens ['[CLS]', 'uneasy', 'mis', '##hma', '##sh', 'of', 'styles', 'and', 'genres', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "21:20:18:BUS-stop:INFO: Token mask [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "21:20:18:BUS-stop:INFO: ***Unlabeled***\n",
      "21:20:18:BUS-stop:INFO: Example 1\n",
      "21:20:18:BUS-stop:INFO: Token ids [  101  2023  2143  1005  1055  3276  2000  5025  6980  2003  1996  2168\n",
      "  2004  2054  4234  1011  3392 19311  2075  1999  1037 12509  2064  2003\n",
      "  2000  5025  4586  1024  1037  3532  1011  1011  2065 25634  1011  1011\n",
      " 20017  1012   102     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "21:20:18:BUS-stop:INFO: Tokens ['[CLS]', 'this', 'film', \"'\", 's', 'relationship', 'to', 'actual', 'tension', 'is', 'the', 'same', 'as', 'what', 'christmas', '-', 'tree', 'flock', '##ing', 'in', 'a', 'spray', 'can', 'is', 'to', 'actual', 'snow', ':', 'a', 'poor', '-', '-', 'if', 'durable', '-', '-', 'imitation', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "21:20:18:BUS-stop:INFO: Token mask [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "task = \"SST-2\"\n",
    "task_path = os.path.join(\"./data\",task)\n",
    "max_seq_length = 64\n",
    "\n",
    "logger.info(\"*******************\")\n",
    "logger.info(\"***Preprocessing***\")\n",
    "logger.info(\"*******************\")\n",
    "task = task.strip()\n",
    "Processor = pp.task_to_processor(task)\n",
    "processor = Processor(task, task_path, tokenizer, max_seq_length) \n",
    "\n",
    "label_list = processor.get_label_list()\n",
    "lab_examples = processor.tsv_to_examples('labeled.tsv')\n",
    "tst_examples = processor.tsv_to_examples('test_with_gold.tsv') \n",
    "unl_examples = processor.tsv_to_examples('unlabeled.tsv')\n",
    "\n",
    "X_lab,y_lab = processor.examples_to_features(lab_examples)\n",
    "X_tst,y_tst = processor.examples_to_features(tst_examples)\n",
    "X_unl,y_unl = processor.examples_to_features(unl_examples)\n",
    "\n",
    "lab_len, unl_len = len(lab_examples), len(unl_examples)\n",
    "num_labels = len(label_list)\n",
    "\n",
    "logger.info('Labeled//Test//Unlabeled matrix shape = {} // {} // {}'.format(\n",
    "    X_lab['input_ids'].shape,X_tst['input_ids'].shape,X_unl['input_ids'].shape))\n",
    "\n",
    "for i in range(2):\n",
    "    logger.info(\"***Train***\")\n",
    "    logger.info (\"Example {}\".format(i))\n",
    "    logger.info (\"Label {}\".format(y_lab[i]))\n",
    "    logger.info (\"Token ids {}\".format(X_lab[\"input_ids\"][i]))\n",
    "    logger.info (\"Tokens {}\".format(tokenizer.convert_ids_to_tokens(X_lab[\"input_ids\"][i])))\n",
    "    #logger.info (\"Token type ids {}\".format(X_lab[\"token_type_ids\"][i]))\n",
    "    logger.info (\"Token mask {}\".format(X_lab[\"attention_mask\"][i]))\n",
    "\n",
    "for i in range(2):\n",
    "    logger.info(\"***Test***\")\n",
    "    logger.info (\"Example {}\".format(i))\n",
    "    logger.info (\"Label {}\".format(y_tst[i]))\n",
    "    logger.info (\"Token ids {}\".format(X_tst[\"input_ids\"][i]))\n",
    "    logger.info (\"Tokens {}\".format(tokenizer.convert_ids_to_tokens(X_tst[\"input_ids\"][i])))\n",
    "    #logger.info (\"Token type ids {}\".format(X_tst[\"token_type_ids\"][i]))\n",
    "    logger.info (\"Token mask {}\".format(X_tst[\"attention_mask\"][i]))\n",
    "\n",
    "for i in range(2):\n",
    "    logger.info(\"***Unlabeled***\")\n",
    "    logger.info (\"Example {}\".format(i))\n",
    "    logger.info (\"Token ids {}\".format(X_unl[\"input_ids\"][i]))\n",
    "    logger.info (\"Tokens {}\".format(tokenizer.convert_ids_to_tokens(X_unl[\"input_ids\"][i])))\n",
    "    #logger.info (\"Token type ids {}\".format(X_unl[\"token_type_ids\"][i]))\n",
    "    logger.info (\"Token mask {}\".format(X_unl[\"attention_mask\"][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "drop_rate = 0.2\n",
    "with strategy.scope():\n",
    "    modeler = ConstructPtModeler(TFModel, Config, pt_model_checkpoint, max_seq_length, \n",
    "                                 num_labels, dense_dropout_prob=drop_rate, word_freeze=True,\n",
    "                                 attention_probs_dropout_prob=drop_rate, hidden_dropout_prob=drop_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:21:02:BUS-stop:INFO: ***********************\n",
      "21:21:02:BUS-stop:INFO: ***Preliminary stage***\n",
      "21:21:02:BUS-stop:INFO: ***********************\n",
      "21:21:02:BUS-stop:DEBUG: Labels in the labeled set mixed evenly like this, ['0', '1', '0', '1', '...'].\n",
      "21:21:02:BUS-stop:DEBUG:  \n",
      "21:21:02:BUS-stop:DEBUG: 0-th run / total 3 runs\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "21:21:34:BUS-stop:DEBUG: For base_i 0, [val_acc,val_loss] = [0.78,0.54].\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "21:22:03:BUS-stop:DEBUG: For base_i 1, [val_acc,val_loss] = [0.88,0.3883].\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "21:22:31:BUS-stop:DEBUG: For base_i 2, [val_acc,val_loss] = [0.74,0.5707].\n",
      "21:22:34:BUS-stop:INFO: In 0-th run, [best_val_acc,best_val_loss] = [0.88,0.3883].\n",
      "21:22:34:BUS-stop:DEBUG:  \n",
      "21:22:34:BUS-stop:DEBUG: 1-th run / total 3 runs\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "21:23:03:BUS-stop:DEBUG: For base_i 0, [val_acc,val_loss] = [0.78,0.5071].\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "21:23:31:BUS-stop:DEBUG: For base_i 1, [val_acc,val_loss] = [0.76,0.52].\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "21:24:01:BUS-stop:DEBUG: For base_i 2, [val_acc,val_loss] = [0.76,0.5222].\n",
      "21:24:05:BUS-stop:INFO: In 1-th run, [best_val_acc,best_val_loss] = [0.78,0.5071].\n",
      "21:24:05:BUS-stop:DEBUG:  \n",
      "21:24:05:BUS-stop:DEBUG: 2-th run / total 3 runs\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "21:24:33:BUS-stop:DEBUG: For base_i 0, [val_acc,val_loss] = [0.78,0.5463].\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "21:24:59:BUS-stop:DEBUG: For base_i 1, [val_acc,val_loss] = [0.82,0.5422].\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "21:25:30:BUS-stop:DEBUG: For base_i 2, [val_acc,val_loss] = [0.78,0.5064].\n",
      "21:25:34:BUS-stop:INFO: In 2-th run, [best_val_acc,best_val_loss] = [0.78,0.5064].\n",
      "21:25:34:BUS-stop:INFO: p_l_conf = [0.5015, 0.5775, 0.5909, ..., 0.9541, 0.9595, 0.9631]\n",
      "21:25:34:BUS-stop:INFO: class distribution of unlabeled data: pred [0.4921 0.5079] -> cali [0.4965 0.5035]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***********************\")\n",
    "logger.info(\"***Preliminary stage***\")\n",
    "logger.info(\"***********************\")\n",
    "preliminary_records = preliminary.run_stage(strategy, modeler, processor, lab_examples, X_unl, rand_seed=GLOBAL_SEED,\n",
    "                                            epochs=30, patience=10, batch_size=16, learning_rate=3e-5, val_ratio=0.5, \n",
    "                                            T=3, n_base=3, verbose=0) #verbose=0/1/2 -> print silent/progress_bar/one_line_per_epoch, \n",
    "p_l_conf, c_u_cali = preliminary.obtain_outputs(preliminary_records, cali_acc_or_f1='f1', bias_lab_or_val='val')\n",
    "\n",
    "#logger.info(\"preliminary_records = {}\".format(preliminary_records))\n",
    "p_l_ = list(np.around(p_l_conf,4))\n",
    "logger.info(\"p_l_conf = [{}, {}, {}, ..., {}, {}, {}]\".format(p_l_[0],p_l_[1],p_l_[2],p_l_[-3],p_l_[-2],p_l_[-1]))\n",
    "logger.info(\"class distribution of unlabeled data: pred {} -> cali {}\".format(\n",
    "    np.around(np.mean(preliminary_records['ulb_dist'],0),4), np.around(c_u_cali,4) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:39:00:BUS-stop:INFO: ****************\n",
      "21:39:00:BUS-stop:INFO: ***Main stage***\n",
      "21:39:00:BUS-stop:INFO: ****************\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bert/pooler/dense/kernel:0', 'encoder/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.6113 - acc: 0.8200\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.6543 - acc: 0.6848\n",
      "21:39:22:BUS-stop:INFO: Epoch 1, s_conf=2.8318, s_class=0.9999, tst_acc=0.6848, tst_loss=0.6543\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.5027 - acc: 0.9000\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.6013 - acc: 0.7611\n",
      "21:39:31:BUS-stop:INFO: Epoch 2, s_conf=2.4574, s_class=0.9985, tst_acc=0.7611, tst_loss=0.6013\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3134 - acc: 0.9600\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.5014 - acc: 0.8018\n",
      "21:39:40:BUS-stop:INFO: Epoch 3, s_conf=1.8869, s_class=0.9979, tst_acc=0.8018, tst_loss=0.5014\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.1723 - acc: 0.9800\n",
      "57/57 [==============================] - 4s 79ms/step - loss: 0.4458 - acc: 0.8051\n",
      "21:39:50:BUS-stop:INFO: Epoch 4, s_conf=1.6072, s_class=0.9852, tst_acc=0.8051, tst_loss=0.4458\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0740 - acc: 0.9900\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.3837 - acc: 0.8386\n",
      "21:39:59:BUS-stop:INFO: Epoch 5, s_conf=1.6482, s_class=0.9952, tst_acc=0.8386, tst_loss=0.3837\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0333 - acc: 0.9900\n",
      "57/57 [==============================] - 5s 79ms/step - loss: 0.3838 - acc: 0.8391\n",
      "21:40:08:BUS-stop:INFO: Epoch 6, s_conf=1.7589, s_class=0.9995, tst_acc=0.8391, tst_loss=0.3838\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0100 - acc: 1.0000\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.4637 - acc: 0.8226\n",
      "21:40:18:BUS-stop:INFO: Epoch 7, s_conf=1.9136, s_class=0.9938, tst_acc=0.8226, tst_loss=0.4637\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0045 - acc: 1.0000\n",
      "57/57 [==============================] - 4s 79ms/step - loss: 0.5107 - acc: 0.8232\n",
      "21:40:27:BUS-stop:INFO: Epoch 8, s_conf=1.9994, s_class=0.9977, tst_acc=0.8232, tst_loss=0.5107\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0027 - acc: 1.0000\n",
      "57/57 [==============================] - 5s 79ms/step - loss: 0.5790 - acc: 0.8144\n",
      "21:40:36:BUS-stop:INFO: Epoch 9, s_conf=2.0496, s_class=0.9995, tst_acc=0.8144, tst_loss=0.579\n",
      "21:40:36:BUS-stop:INFO: ***End training***\n",
      "21:40:36:BUS-stop:INFO: ***Load the model and Evaluate on test data***\n",
      "21:40:36:BUS-stop:INFO: BUS-stop's stop_epoch = 6\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.3838 - acc: 0.8391\n",
      "21:40:41:BUS-stop:INFO: Final tst_acc : 0.8391, tst_loss : 0.3838 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 16\n",
    "n_que = 5\n",
    "\n",
    "logger.info(\"****************\")\n",
    "logger.info(\"***Main stage***\")\n",
    "logger.info(\"****************\")\n",
    "with strategy.scope():\n",
    "    model = modeler.build_model()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08), \n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "\n",
    "steps_per_epoch = lab_len//batch_size\n",
    "queue = deque(n_que*[0], n_que)\n",
    "best_conf, n_pat = np.inf, 0\n",
    "rand_indices = np.arange(lab_len)\n",
    "for epoch in range(1,epochs+1):\n",
    "    \n",
    "    rand_indices = shuffle(rand_indices,random_state=GLOBAL_SEED)\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_indices = rand_indices[step*batch_size:(step+1)*batch_size]\n",
    "        X_bat = {}\n",
    "        for key in X_lab.keys():\n",
    "            X_bat[key] = pp.select_by_index(X_lab[key], batch_indices)\n",
    "        y_bat = pp.select_by_index(y_lab, batch_indices)\n",
    "        model.train_on_batch(X_bat,y_bat)\n",
    "    \n",
    "    trn_loss,trn_acc = model.evaluate(X_lab, y_lab)\n",
    "    tst_loss,tst_acc = model.evaluate(X_tst, y_tst) # \n",
    "    \n",
    "    unl_probs = softmax(model.predict(X_unl),axis=1)\n",
    "    unl_confs = unl_probs.max(1)\n",
    "    unl_dist = unl_probs.mean(0)\n",
    "    \n",
    "    _ids = np.arange(0,unl_len,unl_len/lab_len).astype('int32') # for downsampling\n",
    "    s_conf = euclidean(unl_confs[_ids], p_l_conf)\n",
    "    s_class = 1.-cosine(unl_dist, c_u_cali)\n",
    "    logger.info(\"Epoch {}, s_conf={}, s_class={}, tst_acc={}, tst_loss={}\".format(\n",
    "                    epoch, round(s_conf,4), round(s_class,4), round(tst_acc,4), round(tst_loss,4)))\n",
    "    \n",
    "    if s_conf < best_conf:\n",
    "        n_pat = 0\n",
    "        queue = deque(n_que*[0], n_que)\n",
    "        best_conf = s_conf\n",
    "    else:\n",
    "        n_pat += 1 \n",
    "    \n",
    "    if n_pat < n_que:\n",
    "        if s_class > max(queue):\n",
    "            best_weights = model.get_weights()\n",
    "            stop_epoch = epoch\n",
    "        queue.append(s_class)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "logger.info('***End training***')\n",
    "\n",
    "logger.info('***Load the model and Evaluate on test data***')\n",
    "logger.info(\"BUS-stop's stop_epoch = {}\".format(stop_epoch))\n",
    "model.set_weights(best_weights)\n",
    "tst_loss,tst_acc = model.evaluate(X_tst, y_tst) # \n",
    "logger.info('Final tst_acc : {}, tst_loss : {} \\n'.format(round(tst_acc,4),round(tst_loss,4)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf",
   "language": "python",
   "name": "py36_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
